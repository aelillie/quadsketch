\section{Analysis}
\label{analysis}
%Our analysis of the paper and algorithm goes here
%From the course's intended learning outcomes (ILO):
%"Find, extract and explain results in the algorithms research literature relevant to a given problem"
%"Theoretically analyze the performance of a given algorithmic solution"
This section provides an analysis of properties of the \qs{} algorithm as introduced in \cite{wagner17}, which are deemed interesting and relevant for understanding the algorithm, and thus extend it with an improvement.

%TODO: Do we have more analysis?

\subsection{Why randomly shift hypercube?}
Randomly shifting a hypercube in each dimension is necessary for the \qs{} implementation to maintain guarantees on arbitrary datasets. We hope to obtain a cube in which data points near each other are shifted into the same leaves in the \qt{}. The shifting can have varying practical effects given the randomness of the shifting.
\\
\\
If the dataset is of a known format, randomly shifting it in each dimension could turn out to be less effective than taking a more specific approach. An example could be a dataset in which all the points are very close to each other. A large amount of the points might end up in the same leaves and as such defeat the purpose of using the \qt{} to begin with. Randomly shifting over these points will not be of any significant gain to the problem at hand. One approach could be to spread out the dataset in the sense of scaling the points given some constant to increase the distance between them and hereby obtain a \qt{} of a better quality. The original relative distance is preserved by remembering the scaling constant.

\subsection{Theorems}
The paper introduces three theorems, where \tm{1} is the main theorem covering the basic variant of \qs{}. \tm{2} covers a variant which focuses on minimizing the distortion between all points, and \tm{3} introduces a block variant of \tm{1} inspired by \cite{schmid9}.
\\
\\
Due to some ambiguity between \tm{1} and \tm{2}, it is assumed that for \tm{1} the sketch is created from a single point, and from that point the distances to all other points are preserved up to a factor of 1 $\pm$ $\epsilon$. In \tm{2} it is mentioned that the algorithm is applied recursively in order to preserve the distances between \textit{all} pairs with a high probability.
\\
\\
This ambiguity arises due to several factors. Firstly it is stated in \tm{1} that \textit{foreach point} all distances from that point to all other points are preserved, which would entail that for any point it is possible to retrieve the approximate distance from that point to all others. In \tm{2} it is since highlighted that this theorem makes it possible to approximate the distance between \textit{all pairs of points}. This similarity between the two theorems makes it difficult to exactly analyze the differences between them. Secondly there is no probability guarentee provided in \tm{2}. Without this bound it becomes cumbersome to deduct the cost and the actual value of the recursive process introduced in \tm{2}. Lastly does it not seem logical to introduce \tm{2} as it is not used later in the following theorem, which only builds on top of \tm{1}, nor is it not used throughout the remainder of the paper. 
\\
\\
It could be assumed that \tm{1} creates a sketch where it is possible to preserve the distance between all points similarly to \tm{2}, all though this does not seem likely. The reasons described above argues against it as well as the following property: it is possible to decompress the sketch produced by \tm{1} back into in approximate pointset, whereas this is not possible for \tm{2}. Hence should \tm{1} and \tm{2} both allow for all points to preserve the approximate distances to all other points, it essentially becomes a question of what \tm{2} contributes?
%TODO: Properly review theorem section