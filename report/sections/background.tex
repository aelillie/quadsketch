\section{Background}
\label{background}

\subsection{Research area}
\label{research_area}
%What is the research area?
Many applications use algorithms to find distances between data points in a (typically large) data set in order to extract knowledge based on this. 
%TODO: Specifik reference/eksempel
One example is doing similarity search, or more specifically nearest-neighbor (NN) search, on pictures to recognize letters or numbers. As useful as this is, it still depends on the given data, which are getting increasingly big in size. This is a general problem as even modern machines are not able to process \textit{gigantic} files all at once(i.e. keeping all the data in main memory). Thus another big subject in computer science arises, that is compressing the data in order to fit in memory while trying to preserve as many properties from the data as possible. This research area is what \qs{} deals with, and in particular for high-dimensional data sets.
\\
\\
Naturally one must pay to compress something, which happens in terms of accuracy, i.e. representing the data using only approximations. This also means that there is a trade off between compression and accuracy for the algorithms in this area. That is why compression algorithms usually are measured on these parameters, i.e. accuracy per compression size.
\\
\\
Distance preserving compression algorithms are usually divided into two categories: data-oblivious and data-dependent. The former attempts to achieve guarantees for any data set while the latter attempts to use the extra information about the particular data sets in order to design functions with better performance. Compressed representations makes computation for data analysis algorithms more efficient, which is very desirable in many fields, such as data mining and machine learning.\cite{stan15}

\subsection{State of the art}
\label{state_of_the_art}
Several state of the art algorithms exist for the research area introduced in section \ref{research_area}. \cite{wagner17} mentions \textit{Product Quantization} (\pq{}), which they use as reference state of the art algorithm in their experiments. The \pq{} concept is stated in its original paper, \cite{schmid9}, as such "The idea is to decomposes the space into a Cartesian product of low dimensional subspaces and to quantize each subspace separately", where the term \textit{quantize} refers to the process of constraining an input from a continuous set of values to a discrete set\footnote{E.g. real numbers to integers. See https://en.wikipedia.org/wiki/Quantization 11-04-2018}.

Well known general algorithms and data structures exists for the same purpose, such as \textit{k-means} and \textit{kd-trees}, where the former is a clustering algorithm and the latter is an adaptive data structure for spatial data sets. This is also discussed in \cite{schmid9}, where it is noted that apparently a pure brute-force algorithm outperforms these in practice for high-dimensional data. Further algorithms listed in this paper include "spectral hashing"\cite{weiss8}, "Hamming embedding"\cite{jegou8}, and "FLANN"\cite{muja9}. These will not be investigated further, but are only mentioned for reference.

%From ILO:
%"Identify and formulate precisely (if possible) the algorithmic problem hidden in a given programming task."