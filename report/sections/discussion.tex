\section{Discussion}
\label{discussion}
This section discusses the findings presented in section \ref{results} and evaluates how the results relate to the goals stated in section \ref{intro:problemStatement}. Furthermore possible threats to validity of the results will be discussed.

\subsection{Verification of Performance}
The results in figures \ref{fig:graph sift} and \ref{fig:graph mnist} are used for verifying the results presented in \cite{wagner17}, where results showed that \qs{} outperforms the baseline \grid{} algorithm for the datasets \sift{} and \mnist{} in regards to both accuracy and distortion. In both figures it is clear that \qs{} indeed performs better than \grid{} and therefore confirms this trend. As the precise results in \cite{wagner17} have not been published, no distinct quantitative measure of the ‘similarity’ between the graphs have been used. It is not possible to get a precise reading of each node presented in the original graphs, and as such it is not possible to construct a similarity measure between any two nodes from the original graphs and the graphs created in this paper. 
\\
Apart from these obstacles, it is clear that the graphs very similar, thus indicating that the results presented in \cite{wagner17} are accurate. It should be mentioned for clarity that outliers on the figures are the result of running \qs{} with inappropriate parameters e.g. where there is pruned away to much information. This process follows the original process in \cite{wagner17}. These outliers help verify that the accuracy and distortion of the sketch is indeed controlled by the parameters \textit{L} and $\Lambda$.  

\subsection{Quadsketch and Quadsketch Random}
This subsection will focus on the comparison between \qs{} and \qsr{} as some interesting notions appear from the results. 
\\
Firstly both algorithms obtain approximately the same best scores for most datasets, except for on \mnist{} where \qsr{} achieves slightly better results. 
Secondly \qsr{} seems to have a better average case than \qs{}. 
\\
These result support the theoritical improvements behind \qsr{}, \ref{exp:dist}, where the expected distortion of the points should have better average case performance compared to \qs{}. All datasets tested in this paper confirms this trend (figures \ref{fig:graph sift}, \ref{fig:graph mnist} and \ref{fig:graph clust}) as there is a visable better average case score for both accuracy and distortion when compared with \qs{}.
\\
\\
%TODO, read this section through.
The surprising results from the experiments is that \qsr{} actually has better max scores on the \mnist dataset. This was not expected and points to \qsr{} being a good improvement over \qs{} that its may not only be better in the average case but also better for max scores. This could be due to the fact that it is more probable that points in a real world dataset have bit strings that are uniformly random built of zeros and ones. This implies that \qs{} may never be able to build the original bit string again when there has been pruned away data. In contrast \qsr{} has a probability of guessing the sequence of zeros and ones there was in the original bit string giving a better accuracy and distortion. The reason why this only is apparent for the \mnist{} dataset could be to the number of dimensions in the dataset. Given a higher dimension a larger amount of bits are pruned for each pruning thus resulting in more of the bit string that needs to be replaced. When assuming a real world example of a bit string will have zeros and ones uniformly distributed throughout the bit string then \qsr{} should be able to get better estimate of the original bit string than \qs{} giving a better score for both accuracy and distortion. 


%TODO maybe write about the difference between code and repport

\subsection{Threats to validity}

\subsubsection{Randomness}
\label{disc/threats/randomness}
The comparison of the algorithms shown in \ref{results} show single test runs of the algorithms with different sets of parameters. The seemingly positive results for \qsr{} are inherently subdued to some randomness and could vary on further test runs. The same also applies to \qs{} as it makes use a randomly shifted grid, which also can effect on the outcome of single runs. \qsr{}'s added layer of randomness arguably facilitates the possibility of both worse and better outcomes than \qs{}.
%\subsubsection{Generated dataset}

\subsubsection{Depth of quad in experiments}
\label{disc/threats/depth}
In \cite{wagner17} the experiments on \qs{} and \grid{} runs the parameter $\Lambda$ 1 to 19 and \textit{L} 2 to 20. This project stops the $\Lambda$ at 9 and \textit{L} at 10 because the running time of \qs{} with larger parameters becomes infeasibly large for this project. As the results of this paper already discloses both accuracy and distortion values very close to one, the expected improvements gained by increasing the paramenters beyond the current bound are very limited. However it is possible that increasing the parameters would yield results that could disclose more information or other differences in the algorithms.

\subsubsection{Missing Dimensionality Reduction}
There is a slight offset between the results in \cite{wagner17} and the results in this paper. \cite{wagner17} uses Johnson-Lindenstrauss lemma to reduce the number of dimensions in a dataset to be $d=$\bo{$\epsilon^2\log(n)$}. A slight concern arises because it is unclear if the datasets \mnist{} and \sift{} used in this paper has been preprocessed with Johnson-Lindenstrauss. The concern derives from the \qs{} implementation provided on Github, because this implementation does not apply Johnson-Lindenstrauss as part of the source code. Therefore some investigation has been done in this area, and all though not confirmed it seems that the datasets have not undergone any preprocessing. A supportive argument entails that it would be natural to seperate these two processes, as this yields a more clean implementation of \qs{}. 
\\
In the given case that Johnson-Lindenstrauss has not been applied to the datasets, then the offset is natural (and should be expected), as the number of dimensions in a dataset has a direct impact on the number of bits required to represent a point. If this is not the case, then the results obtained in this paper disagrees with the results obtained in \cite{wagner17}. This is however deemed very unlikely. 