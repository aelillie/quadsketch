\section{Future Work}
\label{futurework}
This section will discuss what future work can be done as an extension to this paper and to \cite{wagner17}. Two new contributions will be shortly introduced and their main obstacles described. 

\subsection{Dynamic Addition}
Through the creation of this project some discussions has taken place regarding the possibilities of dynamically adding new points to a finished sketch. The algorithm introduced in \cite{wagner17} only considers computing a sketch from a static dataset and adding a new point to an existing sketch would therefore require a complete recomputation. Constructing the sketch is a very expensive operation, especially for datasets with many dimensions and a high aspect ratio, as these require a larger tree to obtain appropriate metrics. This high construction time limits the applications of \qs{}. The ability to dynamically add points would allow for \qs{} to be used in other areas such as for instance streaming. Here compact distance presverving sketches are desireable because the amount of memory is limited, and thus many geometric computations are  challenging \cite{ShanM}. As such, the ability to dynamically add new points would be a valuable contribution. 

\subsection{Automatic Parameters}
The implementation provided by Tal Wagner requires several user provided parameters. For users with limited knowledge about the mechanics of the algorithm, this poses a problematic entrance. It is however possible to reduce the number of parameters required, at the cost of an increased running time. This would result in a more user friendly algorithm and would only require the user only to provide the desired distortion guarentee. Given this parameter, the algorithm could calculate the required depth \textit{L} and pruning parameter $\Lambda$ to meet the required guarentee by first calculating the aspect ratio $\Phi$ of the dataset. Calculating $\Phi$ is a simple but very expensive operation as it requires finding the largest and the smallest distance between two points in the given dataset. The cost is easily calculated as it is a matter of comparing each point with every other point, and would thus extend the current running time of \qs{} with \bo{n$^2$} where \textit{n} is the number of points in the dataset. 

\subsection{Optimal Block Size}
Introduced originally in the paper \cite{schmid9} is the idea to split up the datasets into blocks or partitions, which is then individually processed by some quantization method. This method will divide an input vector \textit{x} into \textit{m} distinct subvectors with dimensions \textit{d'} = $\dfrac{d}{m}$ where \textit{d} is the original number of dimensions in \textit{x}. Used with any quantization method, this will produce a large set of centroids from several small sets of centroids: those associated with the subquantizers. This idea is later used in \cite{wagner17} where empirical results show significant differences on the number of bits required to represent a point in the sketch. As this method has shown emprical improvements for \qs{} it is now interesting to investigate why, as this is not proven in \cite{wagner17} and no relatable proof is provided in the original paper \cite{schmid9}. 