\section{Future Work}
\label{futurework}
This section will discuss future work, which can be carried out as an extension to this paper and to \cite{wagner17}. Contributions will be briefly introduced and their main obstacles described. 

\subsection{Dynamic Addition}
Through the creation of this project some discussions has taken place regarding the possibilities of dynamically adding new points to a complete sketch. The algorithm introduced in \cite{wagner17} only considers computing a sketch from a static dataset and adding a new point to an existing sketch would therefore require a complete recomputation. Constructing the sketch is a very expensive operation, especially for datasets with many dimensions and a high aspect ratio, as these require a larger tree to obtain appropriate metrics. The construction time limits the applications of \qs{}. The ability to dynamically add points would allow for \qs{} to be used in other areas such as for instance streaming. Here compact distance preserving sketches are desirable because the amount of memory is limited, and thus many geometric computations are  challenging \cite{ShanM}. As such, the ability to dynamically add new points would be a valuable contribution. 

\subsection{Automatic Parameters}
The implementation of \qs{} requires several user provided parameters. For users with limited knowledge about the mechanics of the algorithm, this poses a problematic entrance. It is however possible to reduce the number of parameters required, at the cost of an increased running time. This would result in a more user friendly algorithm and would only require the user to provide the desired distortion guarantee. Given this parameter, the algorithm could calculate the required depth \textit{L} and pruning parameter $\Lambda$ to meet the required guarantee by first calculating the aspect ratio $\Phi$ of the dataset. Calculating $\Phi$ is a simple but very expensive operation as it requires finding the largest and the smallest distance between two points in the given dataset. The cost is calculated by comparing each point with every other point, and would thus extend the current running time of \qs{} with \bo{n$^2$} where \textit{n} is the number of points in the dataset. Heuristics for setting $\Lambda$ and $L$ could be a beneficial contribution. 

\subsection{Optimal Block Size}
Introduced originally in the paper \cite{schmid9} is the idea to split up the datasets into blocks or partitions, which are then individually processed by some quantization method. As described in \ref{sec:pq} this method will divide an input vector \textit{x} into \textit{m} distinct subvectors with dimensions \textit{D'} = $\dfrac{D}{m}$ where \textit{D} is the original number of dimensions in \textit{x}. Used with any quantization method, this will produce a large set of centroids from several small sets of centroids: those associated with the subquantizers. The related implementation of blocks in \cite{wagner17} show significant differences on the number of bits required to represent a point in the sketch. As this method has shown emprical improvements for \qs{} it would be interesting to investigate precisely why, as this is not proven in \cite{wagner17} and no related proof is provided in the original paper \cite{schmid9}. 