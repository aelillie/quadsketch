\section{Methods}
\label{methods}
%From ILO:
%"Plan and carry out a small-scale investigation of an algorithmic research problem. This investigation could be theoretical, experimental, or both."
This section will cover which methods have been used throughout the project and why the methods have been chosen. Each method is motivated to support the problem definition and the analysis of the \qs{} implementation. The analysis consists of verifying the correctness of the stated results and applying the implementation with other data inputs to verify the relation between \qs{} and the baseline. Furthermore a number of practical tests are conducted to research possible performance gains.

\subsection{Experiment verification}

\subsubsection{Baseline comparison}
The paper compares the \qs{} performance with other compression algorithms, including \textit{Product Quantization} (\pq{}) and a simple baseline algorithm they call Grid. In order to verify their results, a version of Grid has been implemented as baseline to use for replication experiments. The basic concept is to make a range of buckets over each dimension for the point set ps. Each point will have its value for a dimension reduced to the value of the bucket in which the points value lies. By doing so, the value of the point can be stored with fewer bits and thus compress the data set.

\subsubsection{Other datasets}
To ensure proper verification of the results for the \qs{}, the implementation and baseline must be tested on another dataset not included in the paper. If the results from another dataset somewhat match the results given in the paper \cite{wagner17}, it will strengthen the credibility of the practical efficiency of the \qs{} implementation.
\\
\\
The choice of other datasets could provide some additional insight into how the properties of some dataset might impact the optimality of the parameters given to the \qs{}. It might be interesting to feed datasets with different properties to the \qs{}, as results may change from very sparse / spread out datasets to very dense datasets.

\subsection{Experimenting with possible improvements}
A number of experiments will be conducted using different approaches to hopefully obtain improvements to the \qs{}. The approaches and experiments will be elaborated upon below.

\subsubsection{Parameter heuristics}
In the current state the \qs{} has been run with a range of parameters and the results have been analyzed in order to obtain an understanding of which parameters gave the best results for the four presented datasets. To avoid conducting trial runs and result analysis to obtain the optimal parameters for some dataset, it would be desirable to have some heuristics approaching optimal or good parameters for the \qs{} given some dataset.

\subsubsection{Replacement bits}
The replacement of bit values for each dimension which takes place for long edges which have pruned parts of the \qt{}, currently replaces the bits with zero's. This effectively moves the pruned points in a certain direction and causes distortion. Instead of only introducing zero's into the bit string as replacement, inserting random bits might reduce the amount of distortion by pulling the point in different directions for each dimension rather than only distorting it in one direction. Another option could be to insert one's instead of zero's for certain datasets. This would make the distortion 'move' in another direction so to speak.

